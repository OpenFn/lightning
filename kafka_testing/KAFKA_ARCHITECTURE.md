# KAFKA ARCHITECTURE

## Overview

Kafka triggers are implemented via `Lightning.Workflows.Trigger` with the
configuration stored as an embedded schema `.kafka_configuration` represented by
`Lightning.Workflows.Trigger.KafkaConfiguration`.

Each kafka trigger instance gets mapped to a BroadwayKafka pipeline. The
pipelines are supervised by `Lightning.KafkaTriggers.PipelineSupervisor`.

When a Trigger is updated it is added to, removed from or removed from and then
added to the children of the PipelineSupervisor.

The pipeline creates a WorkOrder, Dataclip and Run for each message received
from the Kafka clster. The pipeline also creates a
`Lightning.KafkaTriggers.TriggerKafkaMessageRecord` entry for each message
received. The `TriggerKafkaMessageRecord` helps identify and discard
duplicate messages.

## Detail

### KafkaTriggers.Supervisor

The KafkaTriggers.Supervisor is the top-level supervisor for all the Kafka
functionality. It starts `KafkaTriggers.PipelineSupervisor` and
`KafkaTriggers.EventListener`.

It is also responsible for the initial population of pipelines for the
active Kafka triggers.

### KafkaTriggers.Pipeline

Pipeline uses BroadwayKafka to consume messages from Kakfa. It is configured
via child spec generated by KafkaTriggers.generate_pipeline_child_spec/1.

The configuration of the pipeline is dependent on a combination of per-trigger
configuration (see `Workflows.Trigger.KafkaConfiguration`) and global configuration
that applies to all Kafka triggers.

The global configuration can be used to control the folowing aspects of the
pipeline:

- Whether Kafka triggers are enabled
- The number of messages per second that will be pulled from the cluster.
- The number of consumers in the consumer group. As far as I know, the ideal
  configuration is to have one consumer per partition on the cluster.
- The number of concurrent processors.

See `.env.example` for further details.

The BroadwayKafka pipeline sends the cluster the offset commit upon completion
of the pipeline, **whether the pipeline completed successfully or not**.

The pipeline creates a Workflow, Dataclip and Run for each messages received. It
also creates a TriggerKafkaMessageRecord record. This tracks the combination of
trigger, topic, parition and offset. Each incoming message is checked for an
existing TriggerKafkaMessageRecord that matches and, if so, the message is
discarded.

Errors in message processing (e.g. failure to persist) are handled by marking
the message as failed and then writing some details regarding the message to
the log and to Sentry.

For persistence failures, at a minimum, selected users will receive an email
notifying them of the failure, and suggesting they use the logs to get more
detail. If the instance has `KAFKA_ALTERNATE_STORAGE_ENABLED` set to `yes`, then
a copy of the message will be written to the nominated path, providing someone
with console access  the option to reprocess these messages. For more details,
 see the 'Persisting Failed Messages' subsection found within the
 'Kafka Triggers' secion of `DEPLOYMENT.md`.


### Workflows.Trigger.KafkaConfiguration

`KafkaConfiguration` is a wrapper around the configuration for a Kafka trigger
and, as such, many of its fields are fairly self-explanatory, with two
exceptions:

`initial_offset_reset_policy` determines what happens when a new consumer group
connects to a topic and there is no committed offset. It can have three possible
values `earliest` (start from the earliest message in the topic),
`latest` (start from the latest message in the topic) or a UNIX timestamp
with millsecond precision.

If a timestamp is given the cluster will attempt to
start from the message with the offset closest to the timestamp. Using a
timestamp may be useful for migration scenarios but is has not been tested
outside of a local Kafka cluster so more testing is required.

NOTE: There is a possibility that a consumer group may lose connection with a
Kafka cluster for an extended period of time that exceeds the cluster's
ability to remember the offset for the consumer. When this happens, the cluster
will use whatever `initial_offset_reset_policy` is set to. If the policy is set
to `earliest` or `timestamp`, this may result in the Kafka trigger re-ingesting
messages that have already been processed and will probably result in duplicate
messages as the appropriate TriggerKafkaMessageRecord entires will most likely
have been removed. When Kafka becomes generally available, there should be 
a warning id a user attempts to enable a previously disabled trigger so that
they can be made aware of this potential issue.

Previous implementations of the code tracked timestamps with the Kafka
configuration but this lead to deadlocks when running in multi-node environments
and this was removed to improve performance.

### Workflows

`Workflows.save_workflow/1` has been extended to call
`Triggers.Events.kafka_trigger_updated` if any kafka triggers form part of the
workflow changes.

`Workflows.mark_for_deletion/2` will also call
`Triggers.Events.kafka_trigger_updated` if the workflow in question has any
Kafka triggers associated with it.

The event that is published will be received by `KafkaTriggers.EventListener`.

### KafkaTriggers.EventListener

The `KafkaTriggers.EventListener` listens for events relating to changes to
Kafka trigges and it will update/dd or remove the BroadwayKafka pipeline that
is associated with the affected trigger.

### Lightning.KafkaTriggers.DuplicateTrackingCleanupWorker

The `DuplicateTrackingCleanupWorker` is responsible for removing any
TriggerKafkaMessageRecord records that are older then a specified retention
period. The period can be controlled by setting
`KAFKA_DUPLICATE_TRACKING_RETENTION_SECONDS`. The default value is 3600 seconds.
Setting this value too low will increase the risk of duplicate messages being
processed, during normal operations as well as when resetting the pipeline to 
recover from a persistence error.

## Testing

### Additional test tooling

The following tools are useful if you want non-Elixir tooling that can be used
to validate behaviour:

- `kafka-console-producer.sh` - can be run from within the Kafka container to
  produce a message.
- `kafka-console-consumer.sh` - can be run from within the Kafka container to
  consume a message
- `kcat` - a CLI tool that is useful to tst connections originating from
  outside the container

The Kafka container has a number of CLI
[tools](https://docs.confluent.io/kafka/operations-tools/kafka-tools.html)
that can be useful for testing.

### Viewing triggers that are currently active

```
GenServer.whereis(:kafka_pipeline_supervisor) |> Supervisor.which_children()
```

### Testing locally using the docker Kafka cluster

Create a network so that the cluster can communicate amongst themselves:

```
docker network create kafka-network
```

Start the cluster. Note: The `KAFKA_MEMORY_LIMIT` and `KAFKA_MEMORY_RESERVE`
are set to 1000M, but these should be adjusted to a value that is appropriate
for your system.

```
KAFKA_MEMORY_LIMIT=1000M KAFKA_MEMORY_RESERVE=1000M KAFKA_DEBUG=false docker-compose -f kafka_testing/docker-compose.kafka-testing-cluster.yml up -d
```

The cluster does not auto-provision topics as I found that it does not seem
to play nicely with the cluster (it is probably possible to configure this away).

Before testing, you will need to create the topics on the cluster. This can be
done from any of the three Kafka nodes, by running the following commands from
within the container:

```
kafka-topics.sh --bootstrap-server localhost:9092 --create --topic foo_topic --partitions=3 --replication-factor=3
kafka-topics.sh --bootstrap-server localhost:9092 --create --topic bar_topic --partitions=3 --replication-factor=3
kafka-topics.sh --bootstrap-server localhost:9092 --create --topic baz_topic --partitions=3 --replication-factor=3
kafka-topics.sh --bootstrap-server localhost:9092 --create --topic boz_topic --partitions=3 --replication-factor=3
```

If the command produces an error containing the text
'The target replication factor of 3 cannot be reached because only 2 broker(s) are registered',
that is an indication that the cluster is not properly configured (see the
cluster troubleshooting section below).

To validate that everything is working correctly, run the following from a
**different** Kafka node:

```
kafka-topics.sh --bootstrap-server localhost:9092 --list
```

This should list all the topics that you just created.

Now, you can produce message for the topic - this can be done via
`kafka_console-producer.sh` as detailed elsewhere or you could use kcat:

```
cat message.json | kcat -P -b 127.0.0.1:9094 -t baz_topic
```

The above example is connecting to the Kafka node that exposes port 9094,
but ports 9095 or 9096 can alse be used. If time allows, I will script
a way to automate the population of topics.

Once these are setup, you can create a Kafka trigger via the UI.

You will need to specify the following:

```
Hosts: localhost:9094, localhost:9095, localhost:9096
Topic: Can be any *one* of foo_topic, bar_topic, baz_topic, boz_topic
Group ID: Can be anything, as long as it is unique.
Initial offset reset policy: earliest, latest or a timespam with millisecond precision
SSL: Unselected
SASL Authentication: None
Username: Leave blank
Passord: Leave blank.
```

#### Cluster troubleshooting

Most of the issues I have had so far have been due to configuration issues.
Hopefully, the current container configuration should 'just work'.

If you are not seeing topics show up on other nodes in the cluster, you
can check if the basic netowrk plumbing is working by seeing if you
can connect to other nodes and list their topics.

For example, if kafka-05 is not reflecting the topics but you can see that
the topic exists on kafka-04, you can run the below from kafka-05:

```
kafka-topics.sh --bootstrap-server kafka-04:9092 --list
```

If that returns the expected list of topics, you know that the basic network
topology is working. If the netwrok is not working it should complain about an
unknown host and you should also see an error if you have specified the
incorrect port (e.g. you think kafka-04 is listening to port 9092, but it has
been configured for 9091).

If you make a syntax error when populating `KAFKA_CFG_LISTENERS`, that can
result in the node not being able to communicate - check the syntax carefully.

Another configuration setting to check is `KAFKA_CFG_CONTROLLER_QUORUM_VOTERS`.
This should include all the nodes in the cluster and it should be the same for
all nodes. You may need to check this if you get the
'only ... broker(s) are registered' error or you see that not all the nodes
are getting the topics.

If you are having issues producing to one topic, but the other topics seem to
be ok, you can view the topic in detail:

```
kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic foo_topic
```

The output should be similar to the below:

```
Topic: foo_topic	TopicId: lCC4UrNqRB2pXzjuHOvXxA	PartitionCount: 3	ReplicationFactor: 3	Configs: segment.bytes=1073741824
Topic: foo_topic	Partition: 0	Leader: 3	Replicas: 3,1,2	Isr: 3,1,2
Topic: foo_topic	Partition: 1	Leader: 1	Replicas: 1,2,3	Isr: 1,2,3
Topic: foo_topic	Partition: 2	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1
```

While testing I had a problematic topic where the `Leader` was listed as `None`. I had to
delete the topic using:

```
kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic foo_topic
```

And then I had to recreate it using `--create`.

### Testing using a Confluent Cloud instance

At the time of writing, Confluent Cloud offered a certain number of credits
for new users which can be used to run a local instance. You do need to provide
your credit card details (le sigh), but thus far my usage has been low enough
so that I still have the same number of credits as when I started.

Confluent Cloud is useful for testing SASL authentication and SSL and can
serve as a sanity check.

As Confluent Cloud requires authentication, the most convenient way to use it
with `kcat` is to create a config file at `~/.config/kcat.conf`:

```
bootstrap.servers=my-host.us-west2.gcp.confluent.cloud:9092
security.protocol=sasl_ssl
sasl.mechanism=PLAIN
sasl.username=your-username
sasl.password=your-password
session.timeout.ms=45000
```

Assuming you have created a topic on your Confluent Cloud instance, named
`baz-topic`, you can produce a message using the below:

```
cat message.json | kcat -P -t baz_topic
```

When creating a Trigger via the UI, you will need to select SSL and set
`SASL Authentication`, `Username` and `Password` based on what Confluent
provides.

Confluent provides a nice UI which is useful for setting up topics or
producing the occasional message.
